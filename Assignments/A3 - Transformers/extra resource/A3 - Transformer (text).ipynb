{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3: Transformers (text)\n",
    "\n",
    "In today's assignment, we will take a look at Tranformer. Some of the material in this\n",
    "lab comes from the following online sources:\n",
    "\n",
    "- https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51\n",
    "- https://towardsdatascience.com/how-to-code-the-transformer-in-pytorch-24db27c8f9ec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers and machine learning trends\n",
    "\n",
    "Before the arrival of transformers, CNNs were most often used in the visual domain, while RNNs\n",
    "like LSTMs were most often used in NLP. There were many attempts at crossover, without much real success. Neither approach seemed capable of dealing with very large complex\n",
    "natural language datasets effectively.\n",
    "\n",
    "In 2017, the Transformer was introduced. The paper, \"Attention is all you need,\" has now been\n",
    "cited more than 70,000 times.\n",
    "\n",
    "The main concept in a Transformer is self-attention, which replaces the sequential processing of\n",
    "RNNs and the local processing of CNNs with the ability to adaptively extract arbitrary relationships\n",
    "between different elements of its input, output, and memory state.\n",
    "\n",
    "## Transformer architecture\n",
    "\n",
    "We will use [Frank Odom's implementation of the Transformer in PyTorch](https://github.com/fkodom/transformer-from-scratch/tree/main/src).\n",
    "\n",
    "The architecture of the transformer looks like this:\n",
    "\n",
    "<img src=\"../img/Transformer.png\" title=\"Transformer\" style=\"width: 600px;\" />\n",
    "\n",
    "Here is a summary of the Transformer's details and mathematics:\n",
    "\n",
    "<img src=\"../img/SummaryTransformer.PNG\" title=\"Transformer Details\" style=\"width: 1000px;\" />\n",
    "\n",
    "There are several processes that we need to implement in the model. We go one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention\n",
    "\n",
    "Before Transformers, the standard model for sequence-to-sequence learning was seq2seq, which combines an RNN for encoding with\n",
    "an RNN for decoding. The encoder processes the input and retains important information in a sequence or block of memory,\n",
    "while the decoder extracts the important information from the memory in order to produce an output.\n",
    "\n",
    "One problem with seq2seq is that some information may be lost while processing a long sequence.\n",
    "Attention allows us to focus on specific inputs directly.\n",
    "\n",
    "An attention-based decoder, when we want to produce the output token at a target position, will calculate an attention score\n",
    "with the encoder's memory at each input position. A high score for a particular encoder position indicates that it is more important\n",
    "than another position. We essentially use the decoder's input to select which encoder output(s) should be used to calculate the\n",
    "current decoder output. Given decoder input $q$ (the *query*) and encoder outputs $p_i$, the attention operation calculates dot\n",
    "products between $q$ and each $p_i$. The dot products give the similarity of each pair. The dot products are softmaxed to get\n",
    "positive weights summing to 1, and the weighted average $r$ is calculated as\n",
    "\n",
    "$$r = \\sum_i \\frac{e^{p_i\\cdot q}}{\\sum_j e^{p_j\\cdot q}}p_i .$$\n",
    "\n",
    "We can think of $r$ as an adaptively selected combination of the inputs most relevant to producing an output.\n",
    "\n",
    "### Multi-head self attention\n",
    "\n",
    "Transformers use a specific type of attention mechanism, referred to as multi-head self attention.\n",
    "This is the most important part of the model. An illustration from the paper is shown below.\n",
    "\n",
    "<img src=\"img/MultiHeadAttention.png\" title=\"Transformer\" style=\"width: 600px;\" />\n",
    "\n",
    "The multi-head attention layer is described as:\n",
    "\n",
    "$$\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$$\n",
    "\n",
    "$Q$, $K$, and $V$ are batches of matrices, each with shape <code>(batch_size, seq_length, num_features)</code>.\n",
    "When we are talking about *self* attention, each of the three matrices in\n",
    "each batch is just a separate linear projection of the same input $\\bar{h}_t^{l-1}$.\n",
    "\n",
    "Multiplying the query $Q$ with the key $K$ arrays results in a <code>(batch_size, seq_length, seq_length)</code> array,\n",
    "which tells us roughly how important each element in the sequence is to each other element in the sequence. These dot\n",
    "products are converted to normalized weights using a softmax across rows, so that each row of weights sums to one.\n",
    "Finally, the weight matrix attention is applied to the value ($V$) array using matrix multiplication. We thus get,\n",
    "for each token in the input sequence, a weighted average of the rows of $V$, each of which corresponds to one of the\n",
    "elements in the input sequence.\n",
    "\n",
    "Here is code for the scaled dot-product operation that is part of a multi-head attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as f\n",
    "from torch import Tensor, nn\n",
    "\n",
    "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "    # MatMul operations are translated to torch.bmm in PyTorch\n",
    "    temp = query.bmm(key.transpose(1, 2))\n",
    "    scale = query.size(-1) ** 0.5\n",
    "    softmax = f.softmax(temp / scale, dim=-1)\n",
    "    return softmax.bmm(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A multi-head attention module is composed of several identical\n",
    "*attention head* modules.\n",
    "Each attention head contains three linear transformations for $Q$, $K$, and $V$ and combines them using scaled dot-product attention.\n",
    "Note that this attention head could be used for self attention or another type of attention such as decoder-to-encoder attention, since\n",
    "we keep $Q$, $K$, and $V$ separate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.q = nn.Linear(dim_in, dim_q)\n",
    "        self.k = nn.Linear(dim_in, dim_k)\n",
    "        self.v = nn.Linear(dim_in, dim_k)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple attention heads can be combined with the output concatenation and linear transformation to construct a multi-head attention layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads: int, dim_in: int, dim_q: int, dim_k: int):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_q, dim_k) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "\n",
    "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\n",
    "        return self.linear(\n",
    "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each attention head computes its own transformation of the query, key, and value arrays,\n",
    "and then applies scaled dot-product attention. Conceptually, this means each head can attend to a different part of the input sequence, independent of the others. Increasing the number of attention heads allows the model to pay attention to more parts of the sequence at\n",
    "once, which makes the model more powerful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "To complete the transformer encoder, we need another component, the *position encoder*.\n",
    "The <code>MultiHeadAttention</code> class we just write has no trainable components that depend on a token's position\n",
    "in the sequence (axis 1 of the input tensor). Meaning all of the weight matrices we have seen so far\n",
    "*perform the same calculation for every input position*; that is, we don't have any position-dependent weights.\n",
    "All of the operations so far operate over the *feature dimension* (axis 2). This is good in that the model is compatible with any sequence\n",
    "length. But without *any* information about position, our model is going to be unable to differentiate between different orderings of\n",
    "the input -- we'll get the same result regardless of the order of the tokens in the input.\n",
    "\n",
    "Since order matters (\"Ridgemont was in the store\" has a different\n",
    "meaning from \"The store was in Ridgemont\"), we need some way to provide the model with information about tokens' positions in the input sequence.\n",
    "Whatever strategy we use should provide information about the relative position of data points in the input sequences.\n",
    "In the Transformer, positional information is encoded using trigonometric functions in a constant 2D matrix $PE$:\n",
    "\n",
    "$$PE_{(pos,2i)}=\\sin (\\frac{pos}{10000^{2i/d_{model}}})$$\n",
    "$$PE_{(pos,2i+1)}=\\cos (\\frac{pos}{10000^{2i/d_{model}}}),$$\n",
    "\n",
    "where $pos$ refers to a position in the input sentence sequence and $i$ refers to the position along the embedding vector dimension.\n",
    "This matrix is *added* to the matrix consisting of the embeddings of each of the input tokens:\n",
    "\n",
    "<img src=\"../img/positionalencoder.png\" title=\"Positional Encoder\" style=\"width: 400px;\" />\n",
    "\n",
    "Position encoding can implemented as follows (put this in `utils.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding(seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\")) -> Tensor:\n",
    "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\n",
    "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\n",
    "    phase = pos / (1e4 ** (dim // dim_model))\n",
    "\n",
    "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These sinusoidal encodings allow us to work with arbirary length sequences because the sine and cosine functions are periodic in the range\n",
    "$[-1, 1]$. One hope is that if during inference we are provided with an input sequence longer than any found during training.\n",
    "The position encodings of the last elements in the sequence would be different from anything the model has seen before, but with the\n",
    "periodic sine/cosine encoding, there will still be some similar structure, with the new encodings being very similar to neighboring encodings the model has seen before. For this reason, despite the fact that learned embeddings appeared to perform equally as well, the authors chose\n",
    "this fixed sinusoidal encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The complete encoder\n",
    "\n",
    "The transformer uses an encoder-decoder architecture. The encoder processes the input sequence and returns a sequence of\n",
    "feature vectors or memory vectors, while the decoder outputs a prediction of the target sequence,\n",
    "incorporating information from the encoder memory.\n",
    "\n",
    "First, let's complete the transformer layer with the two-layer feed forward network. Put this in `utils.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(dim_input, dim_feedforward),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(dim_feedforward, dim_input),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a residual module to encapsulate the feed forward network or attention\n",
    "model along with the common dropout and LayerNorm operations (also in `utils.py`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, *tensors: Tensor) -> Tensor:\n",
    "        # Assume that the \"query\" tensor is given first, so we can compute the\n",
    "        # residual.  This matches the signature of 'MultiHeadAttention'.\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the complete encoder! Put this in `encoder.py`. First, the encoder layer\n",
    "module, which comprised a self attention residual block followed by a fully connected residual block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        src = self.attention(src, src, src)\n",
    "        return self.feed_forward(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the Transformer encoder just encapsulates several transformer encoder layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor) -> Tensor:\n",
    "        seq_len, dimension = src.size(1), src.size(2)\n",
    "        src += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            src = layer(src)\n",
    "\n",
    "        return src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The decoder\n",
    "\n",
    "The decoder module is quite similar to the encoder, with just a few small differences:\n",
    "- The decoder accepts two inputs (the target sequence and the encoder memory), rather than one input.\n",
    "- There are two multi-head attention modules per layer (the target sequence self-attention module and the decoder-encoder attention module) rather than just one.\n",
    "- The second multi-head attention module, rather than strict self attention, expects the encoder memory as $K$ and $V$.\n",
    "- Since accessing future elements of the target sequence would be \"cheating,\" we need to mask out future elements of the input target sequence.\n",
    "\n",
    "First, we have the decoder version of the transformer layer and the decoder module itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 6,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_q = dim_k = max(dim_model // num_heads, 1)\n",
    "        self.attention_1 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.attention_2 = Residual(\n",
    "            MultiHeadAttention(num_heads, dim_model, dim_q, dim_k),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.feed_forward = Residual(\n",
    "            feed_forward(dim_model, dim_feedforward),\n",
    "            dimension=dim_model,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        tgt = self.attention_1(tgt, tgt, tgt)\n",
    "        tgt = self.attention_2(tgt, memory, memory)\n",
    "        return self.feed_forward(tgt)\n",
    "\n",
    "    \n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_layers: int = 6,\n",
    "        dim_model: int = 512,\n",
    "        num_heads: int = 8,\n",
    "        dim_feedforward: int = 2048,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.linear = nn.Linear(dim_model, dim_model)\n",
    "\n",
    "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\n",
    "        seq_len, dimension = tgt.size(1), tgt.size(2)\n",
    "        tgt += position_encoding(seq_len, dimension)\n",
    "        for layer in self.layers:\n",
    "            tgt = layer(tgt, memory)\n",
    "\n",
    "        return torch.softmax(self.linear(tgt), dim=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there is not, as of yet, any masked attention implementation here!\n",
    "Making this version of the Transformer work in practice would require at least that.\n",
    "\n",
    "### Putting it together\n",
    "\n",
    "Now we can put the encoder and decoder together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        num_encoder_layers: int = 6,\n",
    "        num_decoder_layers: int = 6,\n",
    "        dim_model: int = 512, \n",
    "        num_heads: int = 6, \n",
    "        dim_feedforward: int = 2048, \n",
    "        dropout: float = 0.1, \n",
    "        activation: nn.Module = nn.ReLU(),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_decoder_layers,\n",
    "            dim_model=dim_model,\n",
    "            num_heads=num_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\n",
    "        return self.decoder(tgt, self.encoder(src))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Letâ€™s create a simple test, as a sanity check for our implementation. We can construct random tensors for the input and target sequences, check that our model executes without errors, and confirm that the output tensor has the correct shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16, 512])\n"
     ]
    }
   ],
   "source": [
    "src = torch.rand(64, 32, 512)\n",
    "tgt = torch.rand(64, 16, 512)\n",
    "out = Transformer()(src, tgt)\n",
    "print(out.shape)\n",
    "# torch.Size([64, 16, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could try implementing masked attention and training this Transformer model on a\n",
    "sequence-to-sequence problem. However, to understand masking, you might first find\n",
    "the [PyTorch Transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "useful. Note that this model is only a Transformer encoder for language modeling, but it uses\n",
    "masking in the encoder's self attention module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
